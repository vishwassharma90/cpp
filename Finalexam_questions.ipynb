{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the main difference between classification and regression Mixture of Gaussians and EM\n",
    "\n",
    "Classification Regression\n",
    "\n",
    "1. Output type:\n",
    "Classification: Discrete (labels)\n",
    "\n",
    "Regression: Continuous (numbers)\n",
    "\n",
    "2. What we have searched?:\n",
    "Classification: Decision boundary\n",
    "\n",
    "Regression: Best fit line\n",
    "\n",
    "3. Evaluation metric:\n",
    "Classification: Accuracy\n",
    "\n",
    "Regression: Sum of squared error\n",
    "\n",
    "MOG - Sum of M individual gaussian distributions.\n",
    "\n",
    "EM - An algorithm works by soft assignment of data points to clusters with a particular probability and used to estimate the distribution parameters. One of the useful algorithms to estimate MOG parameters.\n",
    "\n",
    "MOG is a mixture distribution.\n",
    "\n",
    "EM - algorithm to estimate the parameters of the mixture distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate the main types of Machine learning (Doubt)\n",
    "\n",
    "Depending on the type of data available:\n",
    "1. Supervised\n",
    "2. Unsupervised\n",
    "3. Reinforcement - Rewards and feedback, no data but information about the environment is known\n",
    "4. Semisupervised\n",
    "5. Learning by doing - No examples\n",
    "\n",
    "or\n",
    "\n",
    "1. Classical ML - simple data clear features\n",
    "2. Reinforcement learning\n",
    "3. Ensemble- Quality is an issue\n",
    "4. Neural networks - Complicated data, unclear features and belief is a miracle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each type of learning define its application field (Doubt)\n",
    "\n",
    "1. Supervised learning - Classification and regression\n",
    "\n",
    "** Application- Spam filtering, Sentiment analysis and Fraud detection (Classification)\n",
    "\n",
    "** Application- Stock market forecasts, medical diagnosis (Regression)\n",
    "2. Unsupervised learning - Dimensionality reduction and Clustering\n",
    "\n",
    "** App: Image compression, Market segmentation (Clustering)\n",
    "\n",
    "** App: Recommender systems, Risk management (Dimensionality reduction)\n",
    "3. Reinforcement leanring\n",
    "\n",
    "** Applications: Robotvaccums, self-driving cars, games, automating trading\n",
    "4. Neural network\n",
    "\n",
    "** Applications: Machine translation, Style transfer, Image processing, Speech recognition\n",
    "5. Ensemble\n",
    "\n",
    "** Applications: Search systems, CV, Object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts on Bayes Decision Theory\n",
    "\n",
    "When x is a feature vector describing the property of the input and $C_k$ is the class 'k'.\n",
    "1. Prior probability $P(C_k)$ - probability before seeing the data.\n",
    "2. Conditional probability / Likelihood $P(x|C_k)$ - prob x given(of being) $C_k$\n",
    "3. Posterior probability $P(C_k|x)$\n",
    "\n",
    "**Bayes theorem** \n",
    "\n",
    "$P(C_k|x)= \\frac{P(x|C_k)P(C_k)}{P(x)}$\n",
    "\n",
    "$P(x) = \\sum_i{P(x|C_i)P(C_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes’ Theorem: formula and its interpretation.\n",
    "\n",
    "**Bayes theorem:**\n",
    "\n",
    "$P(C_k|x)= \\frac{P(x|C_k)P(C_k)}{P(x)}$\n",
    "\n",
    "$P(x) = \\sum_i{P(x|C_i)P(C_i)}$\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "Posterior = $\\frac{Likelihood\\times Prior}{Normalization factor}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 1., 0., 0., 1., 0., 1., 0., 1.]),\n",
       " array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "a = np.arange(5)\n",
    "hist, bin_edges = np.histogram(a, density=True)\n",
    "bin_edges\n",
    "plt.hist(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood-Ratio test\n",
    "Optimal decision rule\n",
    "\n",
    "Decide for class k, if posterior prob $P(C_k|x)$ is greated than all other class posterior probability,\n",
    "\n",
    "$P(C_k|x)>P(C_j|x) \\forall j\\neq k$\n",
    "\n",
    "$P(x|C_k)P(C_k)>P(x|C_j)P(C_j) \\forall j\\neq k$\n",
    "\n",
    "$\\frac{P(x|C_k)}{P(x|C_j)} > \\frac{P(C_j)}{P(C_k)} \\forall j\\neq k$\n",
    "\n",
    "where $\\frac{P(C_j)}{P(C_k)}$ is the decision threshold $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is loss, meaning of expected loss \n",
    "\n",
    "## (Doubt- What is loss- misclassifications, meaning of exp loss- average of loss over all decisions and true class. since true class is unknown we reduce the expected loss.)\n",
    "\n",
    "Loss - measure of the consequence or expense of making a mistake in the machine learning task. eg: expense of misclassification.\n",
    "\n",
    "A loss function, or cost function, is a single, overall measure of loss incurred in taking\n",
    "any of the available decisions or actions. Our goal is then to minimize the total loss\n",
    "incurred.\n",
    "\n",
    "Optimal decision is minimizing the loss. But the loss depends on true class which is unknown. Therefore we minimize the expected loss.\n",
    "\n",
    "The decision rule that minimizes the expected loss is the one that assigns each new x to the class j for which the quantity E[L] is minimum.\n",
    "\n",
    "$$E[L] = \\sum_{k} \\sum_j \\int_{R_j} L_{kj} P(x,C_k) dx $$\n",
    "\n",
    "$$E[L] = \\sum_k L_{kj} P(C_k|x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which methods do you know to estimate density (Doubt- Gaussian distribution an estimation method?)\n",
    "\n",
    "1. Parametric representation - Maximum likelihood estimation, Bayesian estimation, Gaussian distribution\n",
    "2. Non-parametric representation \n",
    "3. Mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian or Normal distribution: Formula for one and multi-dimensional cases, properties\n",
    "\n",
    "1. Mahalanobis distance - Quad form\n",
    "2. Shape\n",
    "3. Full covariance\n",
    "4. Diagonal covariance\n",
    "5. Uniform variance\n",
    "6. Marginals of gaussians are gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is difference between parametric and non-parametric methods (Doubt)\n",
    "\n",
    "1. Parametric methods:\n",
    "\n",
    "2. Non parametric methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which parametric approaches do you know?\n",
    "\n",
    "1. ML\n",
    "2. Bayesian learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood approach (ML): Idea, limitations?\n",
    "\n",
    "Idea: Maximise the likelihood function or minimize the negative log likelihood.\n",
    "\n",
    "$P(X|\\theta)$ is the likehood of the data inneed have generated from the probability density with parameters $\\theta$. We assume that the data follows a particular distribution and estimate the parameters of the distribution which maximize the liklihood.\n",
    "\n",
    "Limitation:  \n",
    "1. Overfits to the given dataset and requires more training data to improve the accuracy of the parameters estimated.\n",
    "2. Estimation is biased or overfits, often requires a correction or regularization. But accurate approximation for $N \\rightarrow \\infty$\n",
    "3. ML systematically understimates the true variance of the distribution. \n",
    "\n",
    "$E[\\sigma_{ML}^2] = \\frac{N-1}{N}\\sigma^2$\n",
    "\n",
    "$\\bar\\sigma = \\frac{1}{N-1}\\sum_{n=1}^{N}(x_n-\\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the problem with ML\n",
    "\n",
    "Limitation:  \n",
    "1. Overfits to the given dataset and requires more training data to improve the accuracy of the parameters estimated.\n",
    "2. Estimation is biased or overfits, often requires a correction or regularization. But accurate approximation for $N \\rightarrow \\infty$\n",
    "3. ML systematically understimates the true variance of the distribution. \n",
    "\n",
    "$E[\\sigma_{ML}^2] = \\frac{N-1}{N}\\sigma^2$\n",
    "\n",
    "$\\bar\\sigma = \\frac{1}{N-1}\\sum_{n=1}^{N}(x_n-\\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian learning approach\n",
    "\n",
    "Our uncertainty about the parameters Θ is represented by the probability distribution in the Bayesian approach.\n",
    "\n",
    "Before we observe the data, the parameters are described by a prior density\n",
    "p(Θ), which is typically very broad reflecting the fact that we know little\n",
    "about its true value\n",
    "\n",
    "Once we obtain data, we use the Bayes theorem to find the posterior\n",
    "p(Θ|X). Ideally, we want the data to sharpen the posterior p(Θ|X). Said\n",
    "differently, we want to reduce our uncertainty about the parameters.\n",
    "\n",
    "However, it has be kept in mind that our goal is to estimate the density\n",
    "p(x) or, more precisely, the density p(x|X), the density given the evidence\n",
    "provided by the dataset X\n",
    "\n",
    "When the number of available data increases, the posterior p(Θ|X) tends to sharpen. Therefore, the Bayesian estimate of p(x) will approach the ML estimate for number of samples N → ∞.\n",
    "\n",
    "$p(x|X) = \\frac{\\int p(x|\\theta) L(\\theta) p(\\theta) d\\theta}{\\int L(\\theta) p(\\theta)d\\theta}$\n",
    "\n",
    "The parameter $\\theta$ is treated as random variable and a prior distribution of the parameter is used to obtain the estimate P(x|X) from the dataset X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML vs. Bayesian learning\n",
    "\n",
    "ML: \n",
    "\n",
    "Adv: Simple approach, analytically possible method to estimate the parameters of a distribution.\n",
    "\n",
    "Problem: Overestimates or biased. Requires correction or regularization.\n",
    "As N $\\rightarrow \\infty$ estimate is accurate.\n",
    "\n",
    "Bayesian learning:\n",
    "\n",
    "Adv: General approach, avoids the estimation bias by using a prior.\n",
    "\n",
    "Problem: Need to choose suitable prior. Integral over $\\theta$ is often not possible analytically.\n",
    "However, efficient stochastic sampling methods are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which non-parametric approaches do you know?\n",
    "\n",
    "1. Histogram\n",
    "2. Kernel density estimation (Parzen window / Gaussian kernels)\n",
    "3. K-Nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain each of given in the lecture non-parametric method, highlight their properties and problems\n",
    "\n",
    "Histogram - Slide:3(26)\n",
    "Knn and Kernel methods - Slide:3(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using K-Nearest neighbour\n",
    "\n",
    "From bayes classification,\n",
    "\n",
    "$P(C_k|x)$ = $\\frac {P(x|C_k)P(C_k)}{P(x)}$\n",
    "\n",
    "From K-Nearest Neighbors,\n",
    "\n",
    "$P(C_i)$  = $\\frac{N_i}{N}$\n",
    "\n",
    "$P(x)$  = $\\frac{K}{NV}$\n",
    "\n",
    "$P(x|C_i)$  = $\\frac{K_i}{N_i V}$\n",
    "\n",
    "$P(C_i|x)$ = $\\frac{k_i}{N_iV} \\frac{N_i}{N} \\frac{NV}{k}$\n",
    "\n",
    "K acts as a smoothing factor.\n",
    "\n",
    "Theoretical gurantee: For N$\\rightarrow \\infty$ The error rate in 1 K-NN is not more than the twice the optimal error(Based on true conditional class distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Mixture of Gaussians (MoG) (Doubt)\n",
    "\n",
    "$$p(x|\\theta) = \\sum_{j=1}^{M} p(x|\\theta_j)p(j)$$\n",
    "\n",
    "$$p(x|\\theta) = \\sum_{j=1}^{M} N(x|\\mu_j,\\sigma_{j}^{2})\\pi_j$$\n",
    "\n",
    "$p(x|\\theta_j)$ - mixture component\n",
    "\n",
    "$p(x|\\theta)$ - mixture density\n",
    "\n",
    "$p(j)$ - mixture weight or mixture co-efficients\n",
    "\n",
    "\n",
    "Sum of M individual gaussian distribution\n",
    "\n",
    "\n",
    "Superposition of different linear combination of gaussians with different means and variance and by varying the co-efficients of linear combination can be used to estimate any continuous probability density. This linear combination of gaussians is called mixture of gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a problem of estimation of MoG using Maximum Likelihood\n",
    "\n",
    "1. No direct analytical solution for estimating the parameters.\n",
    "2. Complex gradient function (nonlinear mutual dependencies).\n",
    "3. Optimization of a gaussian depends on all other gaussian.\n",
    "4. Can be solved by iterative numerical optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation MoG: two strategies (Doubt)\n",
    "\n",
    "1. Maximum likelihood estimation (No direct analytical solution)\n",
    "2. Clustering by K-Means\n",
    "3. Clustering by Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering: Idea, advantages and limitations\n",
    "\n",
    "1. Idea: Slide 4(24,26)\n",
    "2. Adv,Limitations: Slide 4(29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Expectation-Maximisation (EM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is difference between k-mean and EM\n",
    "\n",
    "K-Means - hard assignment\n",
    "\n",
    "EM- soft assignment\n",
    "\n",
    "K means\n",
    "\n",
    "Hard assign a data point to one particular cluster on convergence.\n",
    "It makes use of the L2 norm when optimizing (Min {Theta} L2 norm point and its centroid coordinates).\n",
    "\n",
    "EM\n",
    "\n",
    "Soft assigns a point to clusters (so it give a probability of any point belonging to any centroid).\n",
    "It doesn't depend on the L2 norm, but is based on the Expectation, i.e., the probability of the point belonging to a particular cluster. This makes K-means biased towards spherical clusters.\n",
    "\n",
    "\n",
    "In typical EM GMM situations, one does take variance and covariance into account. This is not done in k-means.\n",
    "\n",
    "\n",
    "KMeans clustering essentially corresponds to GMM or MOG estimation with EM algorithm whenever the covariances of the K-Gaussians are set to small fixed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM implementation, technical advices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Discriminant Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give a formalisation of Linear discriminant functions, explain its entries (Doubt)\n",
    "\n",
    "A discriminant function is a function which assigns the new feature set x directly to class k depending on a linear discriminant function. The decision surfaces are hyperplanes. The function is learned from the training set X with target labels T. No explicit modelling of probability densities is carried out. The classification is formulated as comparison of functions:\n",
    "\n",
    "$y_1(x),y_2(x),..,y_k(x)$\n",
    "\n",
    "and \n",
    "\n",
    "Classify x as class k if,\n",
    "\n",
    "$y_k(x)>y_j(x) \\forall j \\neq k$\n",
    "\n",
    "for 2 class problem, decision boundary y(x)=0 is hyperplane, the simple function is the linear function of the input vectors\n",
    "\n",
    "$y(x) = w^T x+ w_0$\n",
    "\n",
    "Normal vector/ weight vector= w\n",
    "\n",
    "Threshold or bias = $w_0$\n",
    "\n",
    "Cut off = - $\\frac{w_0}{||w||}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for linear discriminants for multiple classes\n",
    "\n",
    "1. One-vs-all classifier =K-1 linear binary discriminant functions\n",
    "2. One-vs-one classifier =K(K-1)/2 linear binary discriminant functions\n",
    "3. Single K class discriminat function with K linear function\n",
    "\n",
    "$y_k(x) = w_k^T x + w_{k0}$\n",
    "\n",
    "Each class is defined by its own linear model.\n",
    "\n",
    "For 1 and 2, pure classification result is ambiguous and 3 is the efficient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-squares classification: approach and problems\n",
    "\n",
    "The single k class discriminant function is a combination of k linear functions given by:\n",
    "$y_k(x) =  w_{k}^Tx + w_{k0}$\n",
    "\n",
    "On grouping in vector notation for a single function, \n",
    "$y(x) = \\bar W^T \\bar x$\n",
    "\n",
    "\\begin{align}\n",
    "\\bar W = \\begin{bmatrix} \n",
    "w_{10}&.. & w_{k0} \\\\\n",
    ".. &.. & ..\\\\\n",
    "w_{1D} & .. & w_{kD}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\bar W = \\begin{bmatrix} \n",
    "\\bar w_{1} & \\bar w_2 & \\bar w_3 .. &\\bar w_k \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\bar X = \\begin{bmatrix} \n",
    "x^T_{1} \\\\\n",
    ".. \\\\\n",
    "x^T_{N}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The general form of the classification problem for the entire dataset in discriminant function is: $Y(X) = \\bar X \\bar W$\n",
    "\n",
    "The approach of minimizing  by sum of least squres $\\bar X \\bar W - T$ to find $\\bar W$ is least squres classification.\n",
    "\n",
    "$E(w) = \\frac{1}{2}\\sum_{n=1}^{N} \\sum_{k=1}^{K}(y_k(x_n;w) - t_{kn})^2 $ \n",
    "\n",
    "On derivating with respect to W and equating to zero, the results is\n",
    "\n",
    "$\\bar W = \\bar X^+ T$\n",
    "\n",
    "$y(x) = T^T (\\bar  X^+)^T \\bar x$\n",
    "\n",
    "Problems:\n",
    "\n",
    "1. Sensitive to outliers. error function penalizes predictions that are too correct.\n",
    "2. LS corresponds to ML under the assumption of gaussian conditional distribution. However, in some cases the binary target vector does not correspond to the gaussian distribution. LS is a wrong probabilistic tool in this cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between linear model and generalised linear model (Doubt)\n",
    "\n",
    "1. LM:\n",
    "$y(x) = w^T x+ w_0$\n",
    "\n",
    "2. GLM:\n",
    "$y(x)= g(w^T x+ w_0)$\n",
    "\n",
    "where g(.) may be nonlinear. This function is called activation function.\n",
    "\n",
    "The decision surface corresponds to,\n",
    "\n",
    "$y(x) = const \\leftrightarrow w^T x+ w_0 = const$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalised linear models advantages and limitations\n",
    "\n",
    "Adv:\n",
    "1. The non-linearity in generalized lm overcomes the disadvantages in least squares error approximation and finds better estimates for the function parameters w using iterative methods.\n",
    "2. NL gives more flexibility.\n",
    "3. Can be used to limit the effect of outliers.\n",
    "4. Logistic Sigmoid provides an nice probabilistic interpretation.\n",
    "\n",
    "Dis:\n",
    "1. Least square minimization does not provide the close-form analytical solution hereafter.\n",
    "2. Needs iterative methods like gradient descent to find the parameter w values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic sigmoid activation function\n",
    "\n",
    "$g(a) = \\frac{1}{1+ \\exp(-a)}$\n",
    "\n",
    "Provides a nice probabilistic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent: idea\n",
    "\n",
    "Error function of the GLM:\n",
    "\n",
    "$$E(w) = \\frac{1}{2} \\sum_{n=1}^{N} \\sum_{k=1}^{K} (\\sum_{j=1}^{M} w_{kj}\\phi_j(x_n) - t_{kn})^2$$ \n",
    "\n",
    "This can no longer be minimized in the closed form.\n",
    "\n",
    "Gradient descent steps:\n",
    "\n",
    "1. Iterative minimization.\n",
    "2. Start with a initial guess for the parameters $w_{kj}$\n",
    "3. Move towards the local minimum along the gradient.\n",
    "\n",
    "$$ w_{kj}^{\\tau+1} = w_{kj}^{\\tau} - \\eta \\frac{\\partial E(w)}{\\partial w_{kj}}\\bigg|_{w^{(\\tau)}} $$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "4. Compute the gradient based on the training data. The different ways of computing the gradients are: batch learning and sequential updating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch learning vs. sequential\n",
    "\n",
    "Equation to find the local minimum by following the gradient.\n",
    "\n",
    "$$ w_{kj}^{\\tau+1}= w_{kj}^{\\tau}- \\eta \\frac{\\partial E(w)}{\\partial w_{kj}}\\bigg|_{w_{kj}^{\\tau}}$$\n",
    "1. Batch learning.\n",
    "\n",
    "Compute the gradient based on all training data.\n",
    "\n",
    "$$ w_{kj}^{\\tau+1}= w_{kj}^{\\tau}- \\eta \\frac{\\partial E(w)}{\\partial w_{kj}}\\bigg|_{w_{kj}^{\\tau}}$$\n",
    "\n",
    "2. Sequential updating.\n",
    "\n",
    "Compute the gradient for single datapoint at a time.\n",
    "\n",
    "$$ w_{kj}^{\\tau+1}= w_{kj}^{\\tau}- \\eta \\frac{\\partial E_n(w)}{\\partial w_{kj}}\\bigg|_{w_{kj}^{\\tau}}$$\n",
    "\n",
    "$$ w_{kj}^{\\tau+1}= w_{kj}^{\\tau}- \\eta \\delta_{kn}\\phi_j(x_n)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression properties and limitations\n",
    "Properties:\n",
    "1. Requires fewer parameters than modelling prior and conditional prob.\n",
    "2. Very often used in statistics.\n",
    "3. Directly represent the posterior distribution.\n",
    "4. Error function: cross entropy error function. Optimization leads to unique minimum. But no closed form exists and IRLS.\n",
    "5. Both online and batch optimization exists.\n",
    "\n",
    "Limitations:\n",
    "1. LR systematically overestimates odd ratio when the sample size is less than ~500.\n",
    "\n",
    "Odd ratio: P+/P- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression error function\n",
    "\n",
    "Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
    "\n",
    "Cross entropy error function:\n",
    "$$p(t|w) = \\prod_{n=1}^{N} y_n^{t_n}\\times (1-y_n)^{(1-t_n)}$$\n",
    "\n",
    "$E(w) = -\\ln p(t|w)$\n",
    "\n",
    "$$E(w) = - \\sum_{n=1}^{N} (t_n\\ln y_n + (1-t_n) \\ln (1-y_n))$$\n",
    "\n",
    "where $y_n =\\sigma(w^T (\\phi(x_n))) = P(C_1|\\phi(x_n)) $\n",
    "\n",
    "$t \\in (0,1)$ and $t = (t_1,...t_N)^T$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic sigmoid\n",
    "\n",
    "Logistic sigmoid:\n",
    "\n",
    "$\\sigma(a) = \\frac{1}{1+\\exp(-a)}$\n",
    "\n",
    "Logit function/Inverse:\n",
    "\n",
    "$a = \\ln(\\frac{\\sigma}{1-\\sigma})$ \n",
    "\n",
    "Symmetric:\n",
    "\n",
    "$\\sigma(-a) = 1-\\sigma(a)$\n",
    "\n",
    "Derivative:\n",
    "\n",
    "$\\frac{d\\sigma}{da}= \\sigma(1-\\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta rule\n",
    "Also called LMS rule\n",
    "\n",
    "$$ w_{kj}^{\\tau+1} = w_{kj}^{\\tau} - \\eta (y_k(x_n;w)-t_{kn}) \\phi_j(x_n)$$\n",
    "\n",
    "$$ w_{kj}^{\\tau+1} = w_{kj}^{\\tau} - \\eta \\delta_{kn} \\phi_j(x_n)$$\n",
    "\n",
    "Simply feedback the input data point weighted by the the classification error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients for error function problems and solutions (Doubt)\n",
    "\n",
    "1. No closed form solution available on cross entropy error function.Perform Itervatively reweighted least mean square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Newton-Raphson method?\n",
    "\n",
    "1. Second order gradient descent scheme given by:\n",
    "\n",
    "$$ w^{\\tau+1} = w^{\\tau} - H^{-1} \\nabla E(w)$$\n",
    "\n",
    "H = Hessian matrix = $\\nabla \\nabla E(w)$ is the second order derivative matrix.\n",
    "\n",
    "Properties:\n",
    "1. Local quadratic approximation of the log likelihood.\n",
    "2. Faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which error functions do you know explain their advantages and disadvantages\n",
    "\n",
    "1. Ideal misclassification error function\n",
    "2. Squared error function (used in least square classification)\n",
    "3. Cross entropy error function /Log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ensembles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition behind ensembles (Doubt)\n",
    "\n",
    "1. K classifiers are available. \n",
    "2. The classifiers are independent and their  errors are uncorrelated.\n",
    "3. Each classifier has a classification error probability<0.5 on the training data.\n",
    "4. Then a simple majority vote of all classifiers should have less error than the individual classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give and explain methods for constructing a set of classifiers\n",
    "\n",
    "Constructing set of classifiers- method of obtaining set of classifiers\n",
    "1. CV\n",
    "2. Bagging - Bootstrap aggregation\n",
    "3. Injecting randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give and explain methods for combining a set of classifiers\n",
    "\n",
    "Combining classifiers - methods for combining different classifiers\n",
    "1. Stacking\n",
    "2. Model combination\n",
    "3. Bayesian Model averaging\n",
    "4. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost: Intuition, formalization, algorithm, properties and limitations\n",
    "\n",
    "Important may require detailed explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function, what are the difference between different error functions\n",
    "\n",
    "1. Ideal classification error\n",
    "2. Squared error\n",
    "3. Cross entropy error\n",
    "4. Exponential error\n",
    "5. Hinge error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree: What is CART framework\n",
    "\n",
    "Classification and Regression trees , a framework for Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n",
    "\n",
    "It builds the tree with 6 general questions namely:\n",
    "Keyword: BP,LP,IM\n",
    "\n",
    "1. Branch\n",
    "2. Property\n",
    "3. Leaf\n",
    "4. Prune\n",
    "5. Impure nodes\n",
    "6. Missing attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree: Properties and limitations\n",
    "Properties:\n",
    "1. Simple learning procedure.\n",
    "2. Interpretable results.\n",
    "3. Can be used for metric, nominal and mixed data.\n",
    "4. Fast evaluation.\n",
    "\n",
    "Limitations: (WGFOSE)\n",
    "1. Weak and noisy classifier.\n",
    "2. Do not Generalize too well.\n",
    "3. Training data Fragmentation\n",
    "4. Overtraining and undertraining\n",
    "5. Stability\n",
    "6. Expensive learning procedure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is randomised decision trees\n",
    "\n",
    "Decision tree runtime complexity: $O(DN^2logN)$\n",
    "\n",
    "1. No longer look for globally optimal split\n",
    "2. Instead randomly use subset of K attributes on which to base the split.\n",
    "3. Choose best splitting attribute e.g. by maximizing the information gain (=reducing entropy):\n",
    "\n",
    "\n",
    "$\\triangle E = \\sum_k\\frac{|S_k|}{|S|}\\sum_ip_i log_2(p_i)$\n",
    "\n",
    "Randomised decision tree runtime complexity: $O(KN^2logN)$ ,\n",
    "where K<<D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain how randomised decision tree is used for character recognition application (Doubt)\n",
    "\n",
    "Goal: Classify small 14x20 images of hand written characters or digits into one of the 26/10 classes.\n",
    "\n",
    "Simple binary features:\n",
    "1. Test for individual binary pixel values.\n",
    "2. Organized in a randomized tree.\n",
    "\n",
    "Image patches(Tags):\n",
    "1. Construct a randomly sampled 4x4 patch.\n",
    "2. Construct a randomized tree based on binary single pixel test.\n",
    "3. Each leaf node corresponds to patch class and produces a tag.\n",
    "\n",
    "Representation of digits(Queries):\n",
    "1. Specify the spatial arrangement of tags.\n",
    "2. An images is answered yes if any such structure is found anywhere.\n",
    "\n",
    "How to search for the spatial arrangement:\n",
    "1. Create a second level dt.\n",
    "2. Start for two tags connected by arc.\n",
    "3. Search for the extension of the confirmed query.\n",
    "4. Select the query with max information gain.\n",
    "4. Recurse.\n",
    "\n",
    "Classification:\n",
    "Average estimate of the posterior probabilities is stored in the leafs. Combination of the leaf posterior probabilities will provide the posterior class probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest idea?\n",
    "\n",
    "Create ensemble of many simple decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest, injecting randomness which two main techniques are used for this?\n",
    "\n",
    "1. Bootstrap sampling process\n",
    "2. Random attribute selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest: properties and limitations?\n",
    "\n",
    "Properties:\n",
    "1. Very simple algorithm.\n",
    "2. Reduces overfitting and generalizes well to new data.\n",
    "3. Fast training.\n",
    "4. Extensions available for distance learning and clustering.\n",
    "\n",
    "Limitations:\n",
    "1. Memory consumption. Decision trees require more memory.\n",
    "2. Performs better when the training data is very less.\n",
    "3. Little performance gain, when the training data is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely randomised decision trees: explain how do they work?\n",
    "\n",
    "1. Simply choosing a random query at each node instead of choosing a subset of K random query to base the split.\n",
    "2. O(N) time complexity.\n",
    "\n",
    "\n",
    "1. Tree develops from a classifier to a flexible container structure.\n",
    "2. Query at each node is random selected and the node query determine the structure of the tree.\n",
    "3. The leaf nodes contain the posterior probabilities of the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is difference between trees and ferns? (Doubt)\n",
    "\n",
    "\n",
    "1. Time complexity:\n",
    "\n",
    "Trees: $O(DN^2logN)$ \n",
    "\n",
    "Ferns: $O(MS)$ where M is the number of ferns and S is the number of query in the fern.\n",
    "\n",
    "2. Definition:\n",
    "\n",
    "A fern is a constrained tree where the same binary test is performed at each level of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is perceptron?\n",
    "\n",
    "Perceptron are generlized linear discriminants. It is a linear classifier and makes predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
    "\n",
    "Input: hand-designed features based on common sense.\n",
    "\n",
    "Output:\n",
    "\n",
    "$y = W^T x  +w_0$ -> Linear output\n",
    "\n",
    "$y =\\sigma( W^T x  +w_0)$ -> Logistic output\n",
    "\n",
    "The process of determining the weights w is called learning.\n",
    "\n",
    "Variants:\n",
    "1. Standard perceptron\n",
    "2. Multi layer perceptron\n",
    "\n",
    "Extensions:\n",
    "1. Multi class network for MDReg or MCClassification\n",
    "2. Non linear basis function to handle data which are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron learning?\n",
    "\n",
    "Perceptron is a generalized linear discriminants. The process of determining the weights w of the perceptron is called perceptron learning. The algorithm is simple and process the training case in some permutation. \n",
    "\n",
    "Delta rule or LMS rule\n",
    "\n",
    "Perceptron learning corresponds to the first order (stochastic) gradient descent of quadratic error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron limitations\n",
    "1. Perceptron with fixed hand coded features can solve any separable function perfectly given the input feature vector. \n",
    "2. In some cases, this requires exponential number of features. For example enumerating all possible binary input vector as separate feature units. However, this does not generalize to all cases.\n",
    "3. Example: Linear classifiers cannot solve some tasks like XOR.\n",
    "\n",
    "Solution:\n",
    "1. Non linear classifiers with good feature design.\n",
    "2. Efficient kernels.\n",
    "3. Learn feature representations using Multilayer perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which loss function do you know\n",
    "\n",
    "1. L2 loss - LSRegression\n",
    "2. L1 loss - Median Reg\n",
    "3. Cross entropy loss - Logistic regression\n",
    "4. Hinge loss - SVM\n",
    "5. Softmax loss - Multiclass probabilistic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is overfitting and how to avoid it?\n",
    "\n",
    "1. Overfitting: The model fits well to the training data and the training error is low. However, the test error is large and the model does not fit well previously unseeen data. This is called overfitting and such models generlization capability is less. \n",
    "2. Avoid overfitting by performing regularization techniques. Using regularizers like L1 regularizer or L2 regularizer while computing the error function during training.\n",
    "\n",
    "$E(w) = \\sum_n L(t_n,y(x_n;w))+ \\lambda \\Omega(W)$\n",
    "\n",
    "$ \\Omega(W) = ||W||_F^2$ (L2- regulariser)\n",
    "\n",
    "$L(t_n,y(x_n;w)) = \\sum_n (y(x_n;w)-t_n)^2$ (L2-loss)\n",
    "\n",
    "$E(w) = \\sum_n L(t_n,y(x_n;w))+ \\lambda||w||^2$\n",
    "\n",
    "This is called weight decay in NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is multi-layer perceptron?\n",
    "\n",
    "1. A multilayer perceptron (MLP) is a class of feedforward artificial neural network.\n",
    "2. A MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function.\n",
    "3. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron.\n",
    "4. It can distinguish data that is not linearly separable.\n",
    "5. MLP with two layer network(1- hidden layer) are universal approximators and can approximate any continuous function of a compact domain provided sufficient nodes in the hidden layer. \n",
    "6. The output is given by the below equation:\n",
    "\n",
    "$$y_k(x) = g^{(2)} (\\sum_{i=0}^h W_{ki}^{(2)}g^{(1)}(\\sum_{i=0}^dW_{ij}^{(1)} x_j))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is backpropagation (Doubt) and tell its advantages.\n",
    "\n",
    "BP:\n",
    "1. An algorithm to efficiently calculate the error gradient for each weights in a mlp using incremental analytical differentiation.\n",
    "\n",
    "2. The gradient at a lower layer is calculated from the results of the layer above. The error gradient is propagated from the above layers to the lower layers to calculate the gradient for all weights in the network.\n",
    "\n",
    "3. The general, application independent, name is “reverse-mode differentiation.” It makes deep models computationally tractable.\n",
    "\n",
    "Adv:\n",
    "1. The gradient for all the weights is calculated at once in a single pass.\n",
    "2. Efficient propagation scheme and used to update the weights of the network after evaluating the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graph and how does it used for backpropagation (Doubt)\n",
    "\n",
    "Computational graphs:\n",
    "1. A computational graph is a directed graph where the nodes correspond to operations or variables.\n",
    "2. Variables can feed their value into operations, and operations can feed their output into other operations. 3. This way, every node in the graph defines a function of the variables.\n",
    "4. The values that are fed into the nodes and come out of the nodes are called tensors, which is just a fancy word for a multi-dimensional array. Hence, it subsumes scalars, vectors and matrices as well as tensors of a higher rank.\n",
    "\n",
    "The backward/reverse mode differentiation is used to implement the backpropagation algorithm in a computational graph.\n",
    "1. Convert the NN to a computational graph.\n",
    "2. Each new layer/module just needs to specify how it affects the forward and backward pass.\n",
    "3. Apply reverse mode differentiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Softmax?\n",
    "1. Standard output format for multiclass output.\n",
    "2. Softmax is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities.\n",
    "3. Softmax is often used in neural networks, to map the non-normalized output of a network to a probability distribution over predicted output classes.\n",
    "\n",
    "\n",
    "$$E(W) = - \\sum_{n=1}^{N}  \\sum_{k=1}^{K} \\{I(t_n = k) \\ln \\frac{\\exp(w^Tx_n)}{\\sum_{j}\\exp(w^T_j x_n)}\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between stochastic and batch learning\n",
    "\n",
    "1. Stochastic: Choose one example from the training set and compute the gradient of the weight vector.\n",
    "2. Batch: Process the entire dataset to compute the gradient of each weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatches: idea, advantages and disadvantages\n",
    "\n",
    "Idea: Process only a small batch of the training examples to compute the weight vector.\n",
    "start with a small batch size and increase as the training proceeds.\n",
    "\n",
    "Adv:\n",
    "1. More stable gradients than the stochastic learning and faster than the batch learning.\n",
    "2. Takes adv of the redundancy in the dataset.\n",
    "3. Matrix operations are more efficient than vector operations.\n",
    "\n",
    "Dis;\n",
    "1. Need to normalize the error function with the minibatch size such that same learning rate is used for all minibatches.\n",
    "\n",
    "$E(W) = \\frac{1}{N}(-\\sum_{n=1}^N \\sum_{k=1}^K L(t_n,y(x_n;W)) + \\lambda \\Omega(W))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does learning rate impact on the convergence of the backpropagation algorithm (doubt)\n",
    "\n",
    "Analysis of the convergence of gradient descent.\n",
    "\n",
    "If E is quadratic, the optimal learning rate is provided by the inverse of the hessian matrix.\n",
    "\n",
    "$\\eta_{optimal} = (\\frac{d^2E(W^{\\tau})}{d W^2})^{-1}$\n",
    "\n",
    "Conditions:\n",
    "1. $\\eta = \\eta_{optimal}$ => Error directly descends to the minimum.\n",
    "2. $\\eta < \\eta_{optimal}$ => Error takes small steps towards the minimum error\n",
    "3. $\\eta > \\eta_{optimal}$ => Error bounds to take large steps towards the minimum and keeps jumping around the minimum\n",
    "4. $\\eta >> \\eta_{optimal}$ => Error diverges and does not converge to the minimum error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why momentum and RMSProp are useful? Explain ideas and effects of both algorithms. \n",
    "\n",
    "Momentum -  Technique designed to speed up convergence of first order optimization methods like gradient descent.\n",
    "\n",
    "RMSProp (Root mean square propagation)EFFECT- The RMSprop optimizer restricts the oscillations in the vertical direction. Therefore, we can increase our learning rate and our algorithm could take larger steps in the horizontal direction converging faster. \n",
    "\n",
    "Both offer faster convergence and aims to make optimization less sensitive to the parameter setting.\n",
    "\n",
    "Slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain behaviour of the gradients in a long valley and around a saddle points (Doubt).\n",
    "\n",
    "Long valley: In MD, a long plains produces **zero gradient and obstructs** the process of finding the local maxima and minima. Gradient descent0 fail to find the minimum.\n",
    "\n",
    "Saddle points is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function.\n",
    "A stationary point where the gradient is zero and it is not an extremum. Gradients are struck here because of zero gradient. Max for few weights and min for few weights.\n",
    "\n",
    "Minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous sub-optimal local minima.\n",
    "\n",
    "Saddle point dominate in higher dimensional space. Learning is not stuck but it is very slow and patience is required.\n",
    "\n",
    "Optimizers are helpful in overcoming these situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an idea of shuffling the examples\n",
    "\n",
    "1. Network learn fastest when the most unexpected samples are provided.\n",
    "2. At every step, select samples that are most unfamiliar.\n",
    "3. A large relative error occurs, when the input is not learned by the network, meaning the input has too many information. It is obvious to represent the data to the network. It could be disasterous when the presented data is outlier.\n",
    "\n",
    "Therefore, at each step provide samples that are different from the previous class. Do not provide examples of all class A and then class B. Shuffle them so that the network can learn faster.\n",
    "\n",
    "Useful for: Stochastic gradient descent and minibatches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of data augmentation (Doubt)\n",
    "\n",
    "1. Reduce overfitting by training on more data. \n",
    "2. Robust to the expected changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which kinds of augmentations techniques can be performed on an image\n",
    "\n",
    "1. Cropping\n",
    "2. Zooming\n",
    "3. Flipping\n",
    "4. ColorPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation of input data: how to performed it, why does it helpful? (Doubt)\n",
    "\n",
    "Why:\n",
    "1. Faster convergence\n",
    "2. Easy convergence\n",
    "3. Stability of the weights and biases\n",
    "\n",
    "How:\n",
    "1. The mean of each input variable over the training set is zero.\n",
    "2. Input data is scaled to have the same covariance.\n",
    "3. If possible the data is uncorrelated.\n",
    "\n",
    "In MLP: \n",
    "1. Normalize all inputs so that the input unit has zero mean and unit covariance.\n",
    "2. If possible decorrelate the input variable by PCA also known as KL expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which activation’s functions do you know\n",
    "\n",
    "A neural network is comprised of layers of nodes and learns to map examples of inputs to outputs.\n",
    "\n",
    "For a given node, the inputs are multiplied by the weights in a node and summed together. This value is referred to as the summed activation of the node. The summed activation is then transformed via an activation function and defines the specific output or “activation” of the node.\n",
    "\n",
    "Different activation functions:\n",
    "1. Logistic sigmoid\n",
    "2. Symmetric sigmoids  - tanh curves - Hyperbolic tangent\n",
    "3. ReLu - Rectified linear units\n",
    "4. Linear activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why initialisation of weights is important\n",
    "\n",
    "Why:\n",
    "1. The initial weights affects the training process.\n",
    "2. The weights should activate the sigmoids in the linear region for proper propagation of the gradients.\n",
    "3. For better performance and aid direct learning.\n",
    "4. Bad initializations cause vanishing and exploding gradients.\n",
    "5. The weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network.\n",
    "6. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is batch normalisation\n",
    "\n",
    "Motivation:\n",
    "Optimization works best if all the inputs from a layer are normalized.\n",
    "\n",
    "Procedure:\n",
    "1. Introduce an intermediate layer that centers all the activations of the previous layers per minibatch.\n",
    "2. Perform transformation of the activations and undo the transformations during backpropagating the gradients.\n",
    "3. Complications: centering + normalization should be done at the test time also but the minibatches are gone at that point.\n",
    "\n",
    "Learn the normalization parameter to compensate for the expected bias from the previous layers (Usually a moving average).\n",
    "\n",
    "Effect:\n",
    "1. Much improved convergence. But parameter values are important.\n",
    "2. Widely used method in practice.\n",
    "3. Better results and speed up the convergence.\n",
    "\n",
    "Or:\n",
    "Normalization of the activations in the hidden layer for each minibatch.\n",
    "\n",
    "Batch normalization is a method we can use to normalize the inputs of each layer, in order to fight the internal covariate shift problem.\n",
    "\n",
    "Done in four steps:\n",
    "1. Calculate the batch statistics:\n",
    "\n",
    "batch mean and batch variance\n",
    "\n",
    "$\\mu_B = \\frac{1}{m}(\\sum_{i=1}^m x_i)$\n",
    "\n",
    "$\\sigma_B = \\frac{1}{m}\\sum_{i=1}^m (x_i -\\mu_B)^2$\n",
    "\n",
    "2. Normalize the input:\n",
    "\n",
    "$\\bar x_i = \\frac{x_i-\\mu_B}{\\sqrt{\\in + \\sigma_B^2}}$\n",
    "\n",
    "3. Now scale and shift to get the output of the intermediate layer:\n",
    "\n",
    "$y_i = \\gamma \\bar x_i +\\beta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is dropout (Doubt)\n",
    "\n",
    "Dropout is a regularization technique. The procedure is as follows:\n",
    "\n",
    "1. Randomly switch off certain units during training.\n",
    "2. Change the network architecture for each data point, effectively training different variants of the network architecture.\n",
    "3. When using the trained network, multiply the activations with the probability that the unit was set to zero.\n",
    "4. This procedure greatly improves the performance and avoids overfitting.\n",
    "\n",
    "\n",
    "Drawbacks:\n",
    "1. Cannot apply just before the last classification layer.\n",
    "2. When the network is relatively small and shallow compared to the dataset.\n",
    "3. Dropout initially provides less performance during training and as the training progress the performance increases by reducing overfitting. Therefore, dropout is not effective when the training time is limited for an application.\n",
    "\n",
    "Nowadays batch normalization is used frequently than dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fast)",
   "language": "python",
   "name": "fast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
